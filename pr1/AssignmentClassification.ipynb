{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carora/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/carora/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "(14438, 61252)\n",
      "(14442, 61252)\n",
      "(14438,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import string\n",
    "import nltk as nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "data_df = pd.read_csv('train.dat', sep='\\t', header=None)\n",
    "testdata_df = pd.read_csv('test.dat', sep='\\t', header=None)\n",
    "\n",
    "def cmer(name, c=3):\n",
    "    r\"\"\" Given a name and parameter c, return the vector of c-mers associated with the name\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    size = len(name) + 1    \n",
    "    v = []\n",
    "    for x in range(c, size):\n",
    "        tt = name[x-c:x]\n",
    "        v.append(tt)\n",
    "    \n",
    "    return v\n",
    "\n",
    "def cmerwords(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input)-n+1):\n",
    "        output.append(input[i:i+n])\n",
    "    return output\n",
    "\n",
    "# either will use cmerwords or tfidf with n-grams and will check the performance\n",
    "#need to try euclidean distance\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# clean data : lower case the string, remove punctuation, remove stop words, apply lemma.\n",
    "def clean(data_df, index):\n",
    "    data_df_lower = data_df[index].str.lower()\n",
    "    data_df_punc_free = data_df_lower.apply(lambda x:''.join([i for i in x if i not in exclude]))\n",
    "    data_df_stop = data_df_punc_free.apply(lambda x: ' '.join(k for k in x.split() if k not in stop))\n",
    "    normalized = data_df_stop.apply(lambda x: ' '.join(lemma.lemmatize(word, pos=\"v\") for word in x.split()))\n",
    "    normalized_Stem = normalized.apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split()))\n",
    "    return normalized_Stem\n",
    "\n",
    "def preprocessing(train_df, test_df, ngram_min, ngram_max):\n",
    "    # 1 index being the column 1\n",
    "    cleaned_train = clean(data_df, 1)\n",
    "    train_size = len(cleaned_train)\n",
    "    cleaned_test = clean(testdata_df, 0)\n",
    "    test_size = len(cleaned_test)\n",
    "\n",
    "    combined_arr = np.append(cleaned_train, cleaned_test)\n",
    "    combined_arr_size = len(combined_arr)\n",
    "\n",
    "\n",
    "    # apply tf-idf vectorizer with 1 k-mer. and normalize the vectors\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(norm=None, ngram_range=(ngram_min, ngram_max))\n",
    "    combinedVectorize = vectorizer.fit_transform(combined_arr)\n",
    "    trainedVec = combinedVectorize[0:train_size]\n",
    "    testVec = combinedVectorize[train_size:combined_arr_size]\n",
    "    \n",
    "    return trainedVec, testVec\n",
    "\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "\n",
    "trainedVec, testVec = preprocessing(data_df, testdata_df, 1, 1)\n",
    "\n",
    "csr_l2normalize(trainedVec)\n",
    "csr_l2normalize(testVec)\n",
    "clsTrue = data_df[0]\n",
    "# print(type(cls.values))\n",
    "print(trainedVec.shape)\n",
    "print(testVec.shape)\n",
    "print(clsTrue.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=52\n",
      "highest F1 parameters: k=52 F1:0.6203754788094406\n"
     ]
    }
   ],
   "source": [
    "def splitData(mat, clsTrue, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and clsTrue into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( clsTrue[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = clsTrue[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "def classification(mat, clsTrue, k=3, d=10):\n",
    "    r\"\"\" Classify names using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def classifyy(x, classifier):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        return classifier.predict(x)\n",
    "\n",
    "        \n",
    "    macc = 0.0\n",
    "    for f in range(d):\n",
    "        # split data into training and testing\n",
    "        train, clstr, test, clste = splitData(mat, clsTrue, f+1, d)\n",
    "        # predict the class of each test sample\n",
    "        classifier = KNeighborsClassifier(k)\n",
    "        classifier.fit(train, clstr) \n",
    "        clspr = [ classifyy(test[i,:], classifier) for i in range(test.shape[0]) ]\n",
    "        # compute the accuracy of the prediction\n",
    "        F1 = f1_score(clste, clspr, average='micro')\n",
    "        macc += F1\n",
    "        \n",
    "    return macc/d\n",
    "\n",
    "\n",
    "k=3\n",
    "fileName = \"run.dat\"\n",
    "for k in range(52, 53):\n",
    "    key = \"k=\"+str(k)\n",
    "    print(key)\n",
    "    val = classification(trainedVec, clsTrue.values, k)\n",
    "    f = open(fileName,\"a+\")\n",
    "    f.write(\"highest F1 parameters: \" + key + \" F1:\"  + str(val)+'\\n')\n",
    "    print(\"highest F1 parameters: \" + key + \" F1:\"  + str(val))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14442\n",
      "       0\n",
      "0      5\n",
      "1      5\n",
      "2      1\n",
      "3      2\n",
      "4      1\n",
      "5      5\n",
      "6      5\n",
      "7      5\n",
      "8      1\n",
      "9      5\n",
      "10     5\n",
      "11     4\n",
      "12     4\n",
      "13     5\n",
      "14     5\n",
      "15     1\n",
      "16     5\n",
      "17     1\n",
      "18     5\n",
      "19     4\n",
      "20     3\n",
      "21     1\n",
      "22     4\n",
      "23     3\n",
      "24     1\n",
      "25     5\n",
      "26     4\n",
      "27     4\n",
      "28     1\n",
      "29     1\n",
      "...   ..\n",
      "14412  4\n",
      "14413  3\n",
      "14414  1\n",
      "14415  5\n",
      "14416  5\n",
      "14417  3\n",
      "14418  2\n",
      "14419  5\n",
      "14420  1\n",
      "14421  4\n",
      "14422  5\n",
      "14423  5\n",
      "14424  3\n",
      "14425  1\n",
      "14426  1\n",
      "14427  3\n",
      "14428  4\n",
      "14429  5\n",
      "14430  5\n",
      "14431  2\n",
      "14432  1\n",
      "14433  4\n",
      "14434  3\n",
      "14435  5\n",
      "14436  5\n",
      "14437  3\n",
      "14438  5\n",
      "14439  1\n",
      "14440  5\n",
      "14441  5\n",
      "\n",
      "[14442 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "k=52\n",
    "fileName = \"output_1_52_classification.dat\"\n",
    "\n",
    "classifier = KNeighborsClassifier(k)\n",
    "classifier.fit(trainedVec, clsTrue.values) \n",
    "clsFinal = [ classifier.predict(testVec[i,:]) for i in range(testVec.shape[0]) ]\n",
    "#clsFinal = [ classify(testVec[i,:], trainedVec, cls.values, k) for i in range(testVec.shape[0])]\n",
    "\n",
    "print(len(clsFinal))\n",
    "f= open(fileName,\"w+\")\n",
    "for i in range(len(clsFinal)):\n",
    "     f.write(str(int(clsFinal[i]))+'\\n')\n",
    "f.close() \n",
    "\n",
    "oup = pd.read_csv(fileName, sep='\\t', header=None)\n",
    "\n",
    "print(oup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
